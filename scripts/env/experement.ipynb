{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([[12.,  0.,  0.],\n",
      "        [ 0., 12.,  0.],\n",
      "        [ 0.,  0., 12.]]), tensor([[5., 0., 0.],\n",
      "        [0., 5., 0.],\n",
      "        [0., 0., 5.]])), (tensor([[5., 0., 0.],\n",
      "        [0., 5., 0.],\n",
      "        [0., 0., 5.]]), tensor([[32.,  0.,  0.],\n",
      "        [ 0., 32.,  0.],\n",
      "        [ 0.,  0., 32.]])))\n",
      "tensor([[6., 0., 0.],\n",
      "        [0., 6., 0.],\n",
      "        [0., 0., 6.]])\n",
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def func(x,y):\n",
    "    return sum(x**3+x*y*y + x**3*y+y**6)\n",
    "\n",
    "inputs = (torch.ones(3),torch.ones(3))\n",
    "\n",
    "print(torch.autograd.functional.hessian(func,inputs))\n",
    "\n",
    "def func2(x):\n",
    "    return sum(x**3)\n",
    "\n",
    "inputs = torch.ones(3)\n",
    "\n",
    "print(torch.autograd.functional.hessian(func2,inputs))\n",
    "inputs2 = inputs.clone().detach().requires_grad_(True)\n",
    "out = func2(inputs2)\n",
    "out.backward()\n",
    "print(inputs2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 0., 0.],\n",
      "        [0., 2., 0.],\n",
      "        [0., 0., 2.]])\n",
      "(tensor([[2., 0., 0.],\n",
      "        [0., 2., 0.],\n",
      "        [0., 0., 2.]]), tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "def func(x):\n",
    "    return x**2\n",
    "\n",
    "def func2(x,y):\n",
    "    return (x**2)*y\n",
    "inputs = torch.ones(3)\n",
    "inputs2 = (torch.ones(3),torch.ones(3))\n",
    "out = torch.autograd.functional.jacobian(func,inputs) # []\n",
    "out2 = torch.autograd.functional.jacobian(func2,inputs2) # []\n",
    "print(out)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 0., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 0., 3.]], grad_fn=<ViewBackward>)\n",
      "(tensor([[3., 0., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 0., 3.]], grad_fn=<ViewBackward>), tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], grad_fn=<ViewBackward>))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7fc2847a57c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_derivative_of_vector_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_derivative_of_vector_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwrt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# second1 = torch.autograd.grad(out[0][1], inputs, create_graph=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# out.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7fc2847a57c7>\u001b[0m in \u001b[0;36msecond_derivative_of_vector_func\u001b[0;34m(func, inputs, wrt)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0msecond\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_derivative_of_vector_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "def func(x):\n",
    "    return x**3\n",
    "\n",
    "def func2(x,y):\n",
    "    return (x**3)*y\n",
    "inputs = torch.ones(3,requires_grad=True)\n",
    "inputs2 = (torch.ones(3),torch.ones(3))\n",
    "\n",
    "out2 = torch.autograd.functional.jacobian(func2, inputs2) # []\n",
    "# second = torch.autograd.grad(out[0][0], inputs, create_graph=True)\n",
    "def second_derivative_of_vector_func(func, inputs, wrt=None):\n",
    "    # func - func that takes vector input and return vector\n",
    "    # inputs - vector input or cartage of vectors for func\n",
    "    # wrt - if inputs are cartage, that wrt needs to choose the derivative option [[xx,xy],[yx,yy]]\n",
    "    jacobian = torch.autograd.functional.jacobian(func, inputs, create_graph = True) # []\n",
    "    print(jacobian)\n",
    "    if wrt is not None:\n",
    "        jacobian = jacobian[wrt[0]]\n",
    "        last_shape = inputs[wrt[0]].shape[0]\n",
    "    else:\n",
    "        last_shape = inputs.shape[0]\n",
    "    second = torch.zeros((jacobian.shape[0], jacobian.shape[1], last_shape))\n",
    "    for x in range(jacobian.shape[0]):\n",
    "        for y in range(jacobian.shape[1]):\n",
    "            second[x,y] = torch.autograd.grad(jacobian[x,y], inputs, create_graph=True)[0]\n",
    "    return second\n",
    "out = second_derivative_of_vector_func(func,inputs)\n",
    "out2 = second_derivative_of_vector_func(func2,inputs2,wrt=[0,0])\n",
    "# second1 = torch.autograd.grad(out[0][1], inputs, create_graph=True)\n",
    "# out.backward()\n",
    "# print(second)\n",
    "# print(second1)\n",
    "\n",
    "# print(inputs.grad)\n",
    "print(out)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu102'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
